{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "zwd5xOs3p_hR"
   },
   "outputs": [],
   "source": [
    "#Basic pre-reqs:\n",
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "from torch.autograd import Variable\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import clear_output\n",
    "from celluloid import Camera\n",
    "import pickle as pkl\n",
    "from copy import deepcopy as copy\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "\n",
    "#!pip install celluloid\n",
    "\n",
    "#!export CUDA=cu101 && pip install --upgrade torch-scatter==latest+${CUDA} torch-sparse==latest+${CUDA} -f https://pytorch-geometric.com/whl/torch-1.7.0.html\n",
    "#!pip install --upgrade torch-geometric\n",
    "\n",
    "# Mount your google drive in google colab\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')\n",
    "\n",
    "#%cd '/content/drive/MyDrive/ColabNotebooks/MLAM/GN_ABPs'\n",
    "\n",
    "import models\n",
    "# import simulate\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import networkx as nx\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.functional import F\n",
    "from torch.optim import Adam\n",
    "from torch_geometric.nn import MetaLayer, MessagePassing\n",
    "from torch_geometric.utils import to_networkx\n",
    "\n",
    "from models import OGN, varOGN, make_packer, make_unpacker, get_edge_index\n",
    "\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "from models import get_edge_index\n",
    "\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, OneCycleLR\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as onp\n",
    "onp.random.seed(0)\n",
    "\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "rLsX3VkYuRf5"
   },
   "outputs": [],
   "source": [
    "#############\n",
    "# FUNCTIONS #\n",
    "#############\n",
    "\n",
    "# Loss function: This holds definition of our L1 and KL regularizations:\n",
    "\n",
    "def new_loss(self, g, augment=True, square=False):\n",
    "  if square:\n",
    "      return torch.sum((g.y - self.just_derivative(g, augment=augment))**2)\n",
    "  else:\n",
    "      base_loss = torch.sum(torch.abs(g.y - self.just_derivative(g, augment=augment)))\n",
    "      if test in ['_l1_', '_kl_']:\n",
    "          s1 = g.x[self.edge_index[0]]\n",
    "          s2 = g.x[self.edge_index[1]]\n",
    "          if test == '_l1_':\n",
    "              m12 = self.message(s1, s2)\n",
    "              regularization = 1e-2\n",
    "              #Want one loss value per row of g.y:\n",
    "              normalized_l05 = torch.sum(torch.abs(m12))\n",
    "              return base_loss, regularization * batch * normalized_l05 / n**2 * n\n",
    "          elif test == '_kl_':\n",
    "              regularization = 1\n",
    "              #Want one loss value per row of g.y:\n",
    "              tmp = torch.cat([s1, s2], dim=1)  # tmp has shape [E, 2 * in_channels]\n",
    "              raw_msg = self.msg_fnc(tmp)\n",
    "              mu = raw_msg[:, 0::2]\n",
    "              logvar = raw_msg[:, 1::2]\n",
    "              full_kl = torch.sum(torch.exp(logvar) + mu**2 - logvar)/2.0\n",
    "              return base_loss, regularization * batch * full_kl / n**2 * n\n",
    "      return base_loss\n",
    "\n",
    "\n",
    "# Function to record messages from model\n",
    "\n",
    "def get_messages(ogn):\n",
    "\n",
    "    def get_message_info(tmp):\n",
    "        ogn.cpu()\n",
    "        x = tmp.x\n",
    "        s1 = tmp.x[tmp.edge_index[0]] # (pos, vel, ...) of nodes corresponding to indeces tmp.edge_index[0]\n",
    "        s2 = tmp.x[tmp.edge_index[1]] # same, tmp.edge_index[0] and tmp.edge_index[1] are the nodes forming edges\n",
    "        tmp = torch.cat([s1, s2], dim=1)  # tmp has shape [E, 2 * in_channels]\n",
    "        if test == '_kl_':\n",
    "            raw_msg = ogn.msg_fnc(tmp)\n",
    "            mu = raw_msg[:, 0::2]\n",
    "            logvar = raw_msg[:, 1::2]\n",
    "\n",
    "            m12 = mu\n",
    "        else:\n",
    "            m12 = ogn.msg_fnc(tmp) # it applies the first NN to the variables of the nodes forming edges\n",
    "                                   # the output will have dimension msg_dim (100 in this case)\n",
    "\n",
    "\n",
    "        all_messages = torch.cat((\n",
    "            s1,\n",
    "            s2,\n",
    "            m12), dim=1)\n",
    "\n",
    "        if dim == 2:\n",
    "            columns = [elem%(k) for k in range(1, 3) for elem in 'x%d y%d or%d'.split(' ')] # labels ['x1', 'y1', 'vx1', 'vy1', 'q1', 'm1', 'x2', 'y2', 'vx2', 'vy2', 'q2', 'm2']\n",
    "            columns += ['e%d'%(k,) for k in range(msg_dim)]\n",
    "        elif dim == 3:\n",
    "            columns = [elem%(k) for k in range(1, 3) for elem in 'x%d y%d z%d vx%d vy%d vz%d q%d m%d'.split(' ')]\n",
    "            columns += ['e%d'%(k,) for k in range(msg_dim)]\n",
    "\n",
    "        return pd.DataFrame(\n",
    "            data=all_messages.cpu().detach().numpy(),\n",
    "            columns=columns\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "    msg_info = []\n",
    "    for i, g in enumerate(newtestloader):\n",
    "        msg_info.append(get_message_info(g))\n",
    "\n",
    "    msg_info = pd.concat(msg_info)\n",
    "    msg_info['dx'] = msg_info.x1 - msg_info.x2\n",
    "    msg_info['dy'] = msg_info.y1 - msg_info.y2\n",
    "\n",
    "    if dim == 2:\n",
    "        msg_info['r'] = np.sqrt(\n",
    "            (msg_info.dx)**2 + (msg_info.dy)**2\n",
    "        )\n",
    "    elif dim == 3:\n",
    "        msg_info['dz'] = msg_info.z1 - msg_info.z2\n",
    "        msg_info['r'] = np.sqrt(\n",
    "            (msg_info.dx)**2 + (msg_info.dy)**2 + (msg_info.dz)**2\n",
    "        )\n",
    "\n",
    "    return msg_info\n",
    "\n",
    "def get_self(ogn):\n",
    "\n",
    "    def get_self_info(tmp):\n",
    "        ogn.cpu()\n",
    "        x = tmp.x\n",
    "        s1 = tmp.x[tmp.edge_index[0]] # (pos, vel, ...) of nodes corresponding to indeces tmp.edge_index[0]\n",
    "        s2 = tmp.x[tmp.edge_index[1]] # same, tmp.edge_index[0] and tmp.edge_index[1] are the nodes forming edges\n",
    "        tmp = torch.cat([s1, s2], dim=1)  # tmp has shape [E, 2 * in_channels]\n",
    "        if test == '_kl_':\n",
    "            raw_msg = ogn.msg_fnc(tmp)\n",
    "            mu = raw_msg[:, 0::2]\n",
    "            logvar = raw_msg[:, 1::2]\n",
    "\n",
    "            m12 = mu\n",
    "        else:\n",
    "            m12 = ogn.msg_fnc(tmp) # it applies the first NN to the variables of the nodes forming edges\n",
    "                                   # the output will have dimension msg_dim (100 in this case)\n",
    "\n",
    "            m_aggr = m12.reshape(-1,n-1,msg_dim).sum(dim=1)\n",
    "            pred_v = ogn.node_fnc(torch.cat((x,m_aggr), dim=1))\n",
    "            self_v = ogn.node_fnc(torch.cat((x,torch.zeros(m_aggr.shape)), dim=1))\n",
    "            dist_v = pred_v-self_v\n",
    "\n",
    "        all_messages = torch.cat((\n",
    "            x,\n",
    "            pred_v,\n",
    "            self_v,\n",
    "            dist_v\n",
    "            ), dim=1)\n",
    "\n",
    "        columns = ['x', 'y', 'or', 'totv1', 'totv2', 'selfv3', 'selfv4', 'distv5', 'distv6']\n",
    "\n",
    "        return pd.DataFrame(\n",
    "            data=all_messages.cpu().detach().numpy(),\n",
    "            columns=columns\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "    self_info = []\n",
    "    for i, g in enumerate(newtestloader):\n",
    "        self_info.append(get_self_info(g))\n",
    "\n",
    "    self_info = pd.concat(self_info)\n",
    "\n",
    "    return self_info\n",
    "\n",
    "# Function to visualize network\n",
    "\n",
    "def visualize(h, color, epoch=None, loss=None):\n",
    "    plt.figure(figsize=(7,7))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "\n",
    "    if torch.is_tensor(h):\n",
    "        h = h.detach().cpu().numpy()\n",
    "        plt.scatter(h[:, 0], h[:, 1], s=140, c=color, cmap=\"Set2\")\n",
    "        print(\"isTensor\")\n",
    "        if epoch is not None and loss is not None:\n",
    "            plt.xlabel(f'Epoch: {epoch}, Loss: {loss.item():.4f}', fontsize=16)\n",
    "    else:\n",
    "        print(\"isNotTensor\")\n",
    "        nx.draw_networkx(G, pos=nx.spring_layout(G, seed=42), with_labels=False,\n",
    "                         node_color=color, cmap=\"Set2\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Functions to import LAMMPs data.\n",
    "\n",
    "import glob, re\n",
    "\n",
    "def natural_sort_key(s, _nsre=re.compile('([0-9]+)')):\n",
    "    return [int(text) if text.isdigit() else text.lower()\n",
    "            for text in _nsre.split(s)]\n",
    "\n",
    "def importFiles(filename_glob_pattern, dtype=None, delimiter=None, skip_header=0, usecols=None):\n",
    "    sorted_file_list = sorted(glob.glob(filename_glob_pattern), key=natural_sort_key)\n",
    "    data = []\n",
    "    for file_path in sorted_file_list:\n",
    "        data.append(\n",
    "            np.genfromtxt(file_path, delimiter=delimiter, skip_header=skip_header, dtype=dtype, usecols=usecols))\n",
    "    print(\"Imported {} files.\".format(len(data)))\n",
    "\n",
    "    return data\n",
    "\n",
    "def lammpsDump2numpy(frames):\n",
    "    data = np.asarray(frames);\n",
    "    data_attr = np.array([[[np.concatenate((row[2:6], np.array([0,1]))) for row in col] for col in data]]);\n",
    "    data_accel = np.array([[[row[6:8] for row in col] for col in data]]);\n",
    "\n",
    "    return data_attr, data_accel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "YZ31SBNezWDE"
   },
   "outputs": [],
   "source": [
    "with open('prep20241112-104312.pkl', 'rb') as f:\n",
    "    simulation_data = pkl.load(f)\n",
    "\n",
    "sim = ''\n",
    "data_attr = np.concatenate([simulation_data[\"position\"], simulation_data[\"orientation\"]], axis=3).astype('float32')\n",
    "data_accel = simulation_data[\"velocity\"].astype('float32')\n",
    "dt = simulation_data[\"dt\"]\n",
    "ns, nt, n, dim = simulation_data[\"velocity\"].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "hiSdc8TUzWC6"
   },
   "outputs": [],
   "source": [
    "###########################\n",
    "##    GLOBAL VARIABLES   ##\n",
    "###########################\n",
    "\n",
    "# train on data every #int_subsampling snapshots\n",
    "int_subsampling = 5\n",
    "\n",
    "\n",
    "### OPTIMIZATION ###\n",
    "# every step of the minimization the algorithm take a big graph\n",
    "# made out of #batch disconnected graphs\n",
    "# n: number of nodes (particles).\n",
    "# batch size scales as n^-2 to compensate for the number of messages that goes as n(n-1)\n",
    "batch =  int(64 * (8 / n)**2)\n",
    "\n",
    "# removing edges between particles that are further than a cutoff:\\\n",
    "d_cutoff_TF = 'strict' #'smooth' #'strict' # False # 'smooth' #False/strict/smooth\n",
    "d_cutoff = 5\n",
    "d_cutoff_smth = 0.75\n",
    "\n",
    "\n",
    "# Set up optimizer and training parameters: Use 200 epochs for full version; can use fewer for test.\n",
    "init_lr = 1e-4\n",
    "# With total_epochs=200, batch_per_epoch=2000 it takes ~13h.\n",
    "total_epochs = 20\n",
    "# number of graphs (snapshots) we include in the test data set\n",
    "num_graphs_test  = 500\n",
    "\n",
    "\n",
    "def fermi_func(x):\n",
    "    # x is the square of the distance\n",
    "    return 1/(np.e**((x-d_cutoff**2)/d_cutoff_smth)+1)\n",
    "\n",
    "\n",
    "if d_cutoff_TF == 'smooth':\n",
    "\n",
    "    print('we remove edges of particles following this probability:')\n",
    "    plt.figure()\n",
    "    x_pl = np.linspace(0,10,100)\n",
    "    y_pl = fermi_func(x_pl**2)\n",
    "    plt.plot(x_pl,y_pl)\n",
    "    plt.xlabel('distance')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Custom data loader\n",
    "# HERE WE GENERATE THE CONECTIVITY TENSOR\n",
    "\n",
    "get_edge_index_fully_connected = get_edge_index(n, sim)\n",
    "\n",
    "def get_edge_index_cutoff(X_train,d_cutoff, d_cutoff_TF=True):\n",
    "\n",
    "    if d_cutoff_TF == 'strict':\n",
    "\n",
    "        mat_x = np.tile(X_train[:,0],(len(X_train[:,0]),1))\n",
    "        mat_y = np.tile(X_train[:,1],(len(X_train[:,1]),1))\n",
    "\n",
    "        distances = np.square(mat_x-mat_x.T)+np.square(mat_y-mat_y.T)\n",
    "        distances[distances <= d_cutoff**2 ] = 1\n",
    "        distances[distances > d_cutoff**2 ] = 0\n",
    "        np.fill_diagonal(distances, 0)\n",
    "\n",
    "        edge_index = torch.from_numpy(np.array(np.where(distances)))\n",
    "\n",
    "    elif d_cutoff_TF == 'smooth':\n",
    "\n",
    "        mat_x = np.tile(X_train[:,0],(len(X_train[:,0]),1))\n",
    "        mat_y = np.tile(X_train[:,1],(len(X_train[:,1]),1))\n",
    "\n",
    "        distances = np.square(mat_x-mat_x.T)+np.square(mat_y-mat_y.T)\n",
    "\n",
    "        distances[ fermi_func(distances) >= np.random.rand(*distances.shape) ] = 1\n",
    "        distances[ fermi_func(distances) < np.random.rand(*distances.shape) ] = 0\n",
    "\n",
    "        np.fill_diagonal(distances, 0)\n",
    "\n",
    "        edge_index = torch.from_numpy(np.array(np.where(distances)))\n",
    "\n",
    "    else:\n",
    "        edge_index = get_edge_index_fully_connected\n",
    "\n",
    "    return edge_index\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "S-GsHXE2Td8y"
   },
   "outputs": [],
   "source": [
    "saving_folder = '.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-EbUp2GrzWDG",
    "outputId": "e30b159e-303e-46f4-90ed-7f7447850369"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attr. (X) shape: torch.Size([20000, 20, 3])\n",
      "Accl. (y) shape: torch.Size([20000, 20, 2])\n"
     ]
    }
   ],
   "source": [
    "# Put data in PyTorch format\n",
    "X = torch.from_numpy(np.concatenate([data_attr[i,::int_subsampling] for i in range(0, data_attr.shape[0], 1)]))\n",
    "y = torch.from_numpy(np.concatenate([data_accel[i,::int_subsampling] for i in range(0, data_accel.shape[0], 1)]))\n",
    "\n",
    "X=X.type(torch.FloatTensor)\n",
    "y=y.type(torch.FloatTensor)\n",
    "\n",
    "print(\"Attr. (X) shape: {}\".format(X.shape))\n",
    "print(\"Accl. (y) shape: {}\".format(y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DyjSVQCizWDJ",
    "outputId": "d1ee0631-68f4-4de0-b1aa-8444f8aa2c92"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([15000, 20, 3])\n",
      "torch.Size([5000, 20, 3])\n",
      "torch.Size([15000, 20, 2])\n",
      "torch.Size([5000, 20, 2])\n"
     ]
    }
   ],
   "source": [
    "# Here we divide train and test data, by default it selects 25% of the data as\n",
    "# test data. It seems that this is the last 25% of the systems time evolution.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=False)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3NYWOv06zWDK"
   },
   "source": [
    "# **Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "g9IsisRWzWDM"
   },
   "outputs": [],
   "source": [
    "\n",
    "# L1 regularization\n",
    "\n",
    "aggr = 'add'\n",
    "hidden = 300\n",
    "test = '_l1_'\n",
    "\n",
    "#This test applies an explicit bottleneck:\n",
    "msg_dim = 100\n",
    "\n",
    "n_f = data_attr.shape[3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "ml6ZDUckE_k5"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Initiate the model\n",
    "\n",
    "#######\n",
    "# GPU #\n",
    "#######\n",
    "\n",
    "# if test == '_kl_':\n",
    "#     ogn = varOGN(n_f, msg_dim, dim, dt=0.1, hidden=hidden, edge_index=get_edge_index(n, sim), aggr=aggr).cuda()\n",
    "# else:\n",
    "#     ogn = OGN(n_f, msg_dim, dim, dt=0.1, hidden=hidden, edge_index=get_edge_index(n, sim), aggr=aggr).cuda()\n",
    "\n",
    "# messages_over_time = []\n",
    "# ogn = ogn.cuda()\n",
    "\n",
    "# Test the model\n",
    "# _q = Data(\n",
    "#     x=X_train[0].cuda(),\n",
    "#     edge_index=edge_index.cuda(),\n",
    "#     y=y_train[0].cuda())\n",
    "# ogn(_q.x, _q.edge_index), ogn.just_derivative(_q).shape, _q.y.shape, ogn.loss(_q),\n",
    "\n",
    "\n",
    "\n",
    "#######\n",
    "# CPU #\n",
    "#######\n",
    "\n",
    "if test == '_kl_':\n",
    "    ogn = varOGN(n_f, msg_dim, dim, dt=0.1, hidden=hidden, edge_index=get_edge_index(n, sim), aggr=aggr).cpu()\n",
    "else:\n",
    "    ogn = OGN(n_f, msg_dim, dim, dt=0.1, hidden=hidden, edge_index=get_edge_index(n, sim), aggr=aggr).cpu()\n",
    "\n",
    "messages_over_time = []\n",
    "self_over_time = []\n",
    "ogn = ogn.cpu()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "3Z9X59x_zWDN"
   },
   "outputs": [],
   "source": [
    "\n",
    "# # Test the model\n",
    "# _q = Data(\n",
    "#     x=X_train[0].cpu(),\n",
    "#     edge_index=edge_index.cpu(),\n",
    "#     y=y_train[0].cpu())\n",
    "# ogn(_q.x, _q.edge_index), ogn.just_derivative(_q).shape, _q.y.shape, ogn.loss(_q),\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3zp5Sjw2zWDO",
    "outputId": "4574699e-97e6-4468-c205-973cf01ea6e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computation time:  2.0542144775390625\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "## Set up training\n",
    "\n",
    "time_1 = time.time()\n",
    "\n",
    "# one minibatch has #batch disconnected graphs\n",
    "trainloader = DataLoader(\n",
    "    [Data(\n",
    "        Variable(X_train[i]),\n",
    "        edge_index=get_edge_index_cutoff(X_train[i],d_cutoff,d_cutoff_TF=d_cutoff_TF),\n",
    "        y=Variable(y_train[i])) for i in range(len(y_train))],\n",
    "    batch_size=batch,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "\n",
    "testloader = DataLoader(\n",
    "    [Data(\n",
    "        X_test[i],\n",
    "        edge_index=get_edge_index_cutoff(X_test[i],d_cutoff,d_cutoff_TF=d_cutoff_TF),\n",
    "        y=y_test[i]) for i in range(len(y_test))],\n",
    "    batch_size=1024,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "time_2 = time.time()\n",
    "\n",
    "print('computation time: ', time_2-time_1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'DataLoader' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainloader\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'DataLoader' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "trainloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fcEkf9FizWDP",
    "outputId": "32ff58eb-79dd-4ba3-cf89-083f96abb00a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 10\n",
      "len(y_train) 15000\n",
      "number of edges in a batch in the fully conencted case: n*(n-1)*batch  3800\n",
      "number of edges in this batch:  torch.Size([2, 80])\n",
      "number of edges in a batch in the fully conencted case: n*(n-1)*batch  3800\n",
      "number of edges in this batch:  torch.Size([2, 100])\n",
      "number of edges in a batch in the fully conencted case: n*(n-1)*batch  3800\n",
      "number of edges in this batch:  torch.Size([2, 92])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('batch',batch)\n",
    "print('len(y_train)',len(y_train))\n",
    "# print('ginput.batch',ginput.batch)\n",
    "\n",
    "\n",
    "i=0\n",
    "for ginput in trainloader:     # Trainloader has the 1500 samples (pos vel edge index, ...).\n",
    "    if i >= 3:\n",
    "        break\n",
    "    print('number of edges in a batch in the fully conencted case: n*(n-1)*batch ', n*(n-1)*batch)\n",
    "    print('number of edges in this batch: ',np.shape(ginput.edge_index))\n",
    "\n",
    "    i+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n1sOPFwqF6mV",
    "outputId": "44958a89-4c25-468e-83a2-94b72d630ef8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15000\n",
      "5000\n",
      "links of fully connected graph:  380\n",
      "Data(x=[20, 3], edge_index=[2, 2], y=[20, 2])\n",
      "Data(x=[20, 3], edge_index=[2, 2], y=[20, 2])\n"
     ]
    }
   ],
   "source": [
    "print(len(trainloader.dataset))\n",
    "print(len(testloader.dataset))\n",
    "\n",
    "# edge_index has dimensions [2, num_nodes^2 - num_nodes]\n",
    "print('links of fully connected graph: ',n*(n-1))\n",
    "print(trainloader.dataset[0])\n",
    "print(testloader.dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cSR9JpfdzWDR",
    "outputId": "bcb24444-4f71-4f59-f64d-6b2fba8f9b62"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_per_epoch 1500\n",
      "batch 10\n",
      "batch*batch_per_epoch 15000\n",
      "len(trainloader.dataset) 15000\n",
      "len(y_train) 15000\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# We'll use OneCycleLR for fast training:\n",
    "ogn = ogn.to(device)\n",
    "opt = torch.optim.Adam(ogn.parameters(), lr=init_lr, weight_decay=1e-8)\n",
    "\n",
    "# batch_per_epoch = int(1000*10 / (batch/32.0))\n",
    "batch_per_epoch = int(len(trainloader.dataset)/batch)\n",
    "\n",
    "print('batch_per_epoch',batch_per_epoch)\n",
    "print('batch',batch)\n",
    "print('batch*batch_per_epoch',batch*batch_per_epoch)\n",
    "print('len(trainloader.dataset)',len(trainloader.dataset))\n",
    "print('len(y_train)',len(y_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v7RSJSoazWDS"
   },
   "outputs": [],
   "source": [
    "\n",
    "sched = OneCycleLR(opt, max_lr=init_lr,\n",
    "                  steps_per_epoch=batch_per_epoch,#len(trainloader),\n",
    "                  epochs=total_epochs, final_div_factor=1e5)\n",
    "\n",
    "epoch = 0\n",
    "\n",
    "# Organize the recording of messages over time This is for fitting the forces, and extracting laws:\n",
    "\n",
    "# test_idxes = onp.random.randint(0, len(X_test), num_graphs_test)\n",
    "test_idxes = onp.arange(0, len(X_test),1)\n",
    "\n",
    "#Record messages over test dataset here:\n",
    "# 1000 snapshots taken at random (if 4 particles this is 12000 messages)\n",
    "\n",
    "newtestloader = DataLoader(\n",
    "    [Data(\n",
    "        X_test[i],\n",
    "        edge_index=get_edge_index_cutoff(X_test[i],d_cutoff,d_cutoff_TF=d_cutoff_TF),\n",
    "        y=y_test[i]) for i in test_idxes],\n",
    "    batch_size=len(X_test),\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# idxarr=-1*np.ones((11,151), int);\n",
    "\n",
    "### Train the model:\n",
    "# Training loop\n",
    "recorded_models = []\n",
    "recorded_times = []\n",
    "recorded_loss = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nMsFnm-51Q1i",
    "outputId": "2b78e3e3-b27c-4776-b926-226882bb6bb1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cur_loss 521.162223470052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r  5%|▌         | 1/20 [00:12<03:50, 12.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cur_loss 515.9032039713542\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r 10%|█         | 2/20 [00:23<03:25, 11.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cur_loss 498.2180278320312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r 15%|█▌        | 3/20 [00:33<03:08, 11.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cur_loss 491.2009754557292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r 20%|██        | 4/20 [00:44<02:56, 11.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cur_loss 489.1072786783854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r 25%|██▌       | 5/20 [00:55<02:43, 10.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cur_loss 487.17599814453126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r 30%|███       | 6/20 [01:06<02:33, 10.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cur_loss 485.21411500651044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r 35%|███▌      | 7/20 [01:17<02:21, 10.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cur_loss 483.9709753580729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r 40%|████      | 8/20 [01:28<02:10, 10.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cur_loss 482.99035419921876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r 45%|████▌     | 9/20 [01:38<01:59, 10.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cur_loss 481.9956982747396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r 50%|█████     | 10/20 [01:49<01:48, 10.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cur_loss 481.33308639322917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r 55%|█████▌    | 11/20 [02:00<01:37, 10.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cur_loss 480.7428459309896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r 60%|██████    | 12/20 [02:11<01:26, 10.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cur_loss 480.25953181966145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r 65%|██████▌   | 13/20 [02:22<01:15, 10.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cur_loss 479.93355696614583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r 70%|███████   | 14/20 [02:32<01:04, 10.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cur_loss 479.6402754557292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r 75%|███████▌  | 15/20 [02:43<00:53, 10.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cur_loss 479.44266015625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r 80%|████████  | 16/20 [02:54<00:43, 10.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cur_loss 479.2727662760417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r 85%|████████▌ | 17/20 [03:05<00:32, 10.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cur_loss 479.14536477864584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r 90%|█████████ | 18/20 [03:16<00:21, 10.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cur_loss 479.0643981119792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r 95%|█████████▌| 19/20 [03:27<00:10, 10.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cur_loss 479.03421119791665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [03:37<00:00, 10.90s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "start_time = time.time()\n",
    "for epoch in tqdm(range(epoch, total_epochs)):\n",
    "    ogn.to(device)\n",
    "    total_loss = 0.0\n",
    "    i = 0\n",
    "    num_items = 0\n",
    "    while i < batch_per_epoch:\n",
    "        for ginput in trainloader:     # Trainloader has the 1500 samples (pos vel edge index, ...).\n",
    "            if i >= batch_per_epoch:\n",
    "                break\n",
    "            opt.zero_grad()\n",
    "            ginput.x = ginput.x.to(device)\n",
    "            ginput.y = ginput.y.to(device)\n",
    "            ginput.edge_index = ginput.edge_index.to(device)\n",
    "            ginput.batch = ginput.batch.to(device)\n",
    "            if test in ['_l1_', '_kl_']:\n",
    "                loss, reg = new_loss(ogn, ginput, square=False)\n",
    "                ((loss + reg)/int(ginput.batch[-1]+1)).backward()\n",
    "            else:\n",
    "                loss = ogn.loss(ginput, square=False)\n",
    "                (loss/int(ginput.batch[-1]+1)).backward()\n",
    "            opt.step()\n",
    "            sched.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "#             idxarr[epoch,i]= ginput.x[0,4]\n",
    "\n",
    "            i += 1\n",
    "            print(\"Epoch: {} ({}%) - {}\".format(epoch, 100*i/batch_per_epoch, ginput.x[0,1]), flush=True, end='\\r')\n",
    "            num_items += int(ginput.batch[-1]+1)\n",
    "\n",
    "    cur_loss = total_loss/num_items\n",
    "    print('cur_loss',cur_loss)\n",
    "    cur_msgs = get_messages(ogn)\n",
    "    cur_msgs['epoch'] = epoch\n",
    "    cur_msgs['loss'] = cur_loss\n",
    "    messages_over_time.append(cur_msgs)\n",
    "\n",
    "    # cur_self = get_self(ogn)\n",
    "    # cur_self['epoch'] = epoch\n",
    "    # cur_self['loss'] = cur_loss\n",
    "    # self_over_time.append(cur_self)\n",
    "\n",
    "    ogn.cpu()\n",
    "\n",
    "    recorded_models.append(ogn.state_dict()) # it saves #epoch copies of the last model!!!\n",
    "    recorded_times.append(epoch)\n",
    "    recorded_loss.append(cur_loss)\n",
    "\n",
    "    torch.save(ogn.state_dict(), os.path.join(saving_folder, 'epoch-{0}.pth'.format(epoch)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "83GFZUrFVWlj",
    "outputId": "d8471047-6839-4218-f11b-b2cda4d3a044"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-30-be53b500a000>:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load('epoch-19.pth')\n",
      "  0%|          | 0/9 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cur_loss 480.1299814127604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r 11%|█         | 1/9 [00:11<01:31, 11.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cur_loss 479.96166842447917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r 22%|██▏       | 2/9 [00:22<01:17, 11.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cur_loss 479.87812565104167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r 33%|███▎      | 3/9 [00:33<01:05, 10.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cur_loss 479.729312890625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r 44%|████▍     | 4/9 [00:43<00:54, 10.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cur_loss 479.6389823242188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r 56%|█████▌    | 5/9 [00:54<00:43, 10.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cur_loss 479.68035185546876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r 67%|██████▋   | 6/9 [01:05<00:32, 10.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cur_loss 479.55599560546875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r 78%|███████▊  | 7/9 [01:16<00:21, 10.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cur_loss 479.45494622395836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r 89%|████████▉ | 8/9 [01:27<00:10, 10.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cur_loss 479.4047953450521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [01:38<00:00, 10.91s/it]\n"
     ]
    }
   ],
   "source": [
    "ogn = ogn.to(device)\n",
    "opt = torch.optim.Adam(ogn.parameters(), lr=init_lr, weight_decay=1e-8)\n",
    "checkpoint = torch.load('epoch-19.pth')\n",
    "ogn.load_state_dict(checkpoint)\n",
    "\n",
    "start_time = time.time()\n",
    "for epoch in tqdm(range(epoch+1, 30)):\n",
    "    ogn.to(device)\n",
    "    total_loss = 0.0\n",
    "    i = 0\n",
    "    num_items = 0\n",
    "    while i < batch_per_epoch:\n",
    "        for ginput in trainloader:     # Trainloader has the 1500 samples (pos vel edge index, ...).\n",
    "            if i >= batch_per_epoch:\n",
    "                break\n",
    "            opt.zero_grad()\n",
    "            ginput.x = ginput.x.to(device)\n",
    "            ginput.y = ginput.y.to(device)\n",
    "            ginput.edge_index = ginput.edge_index.to(device)\n",
    "            ginput.batch = ginput.batch.to(device)\n",
    "            if test in ['_l1_', '_kl_']:\n",
    "                loss, reg = new_loss(ogn, ginput, square=False)\n",
    "                ((loss + reg)/int(ginput.batch[-1]+1)).backward()\n",
    "            else:\n",
    "                loss = ogn.loss(ginput, square=False)\n",
    "                (loss/int(ginput.batch[-1]+1)).backward()\n",
    "            opt.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "#             idxarr[epoch,i]= ginput.x[0,4]\n",
    "\n",
    "            i += 1\n",
    "            print(\"Epoch: {} ({}%) - {}\".format(epoch, 100*i/batch_per_epoch, ginput.x[0,1]), flush=True, end='\\r')\n",
    "            num_items += int(ginput.batch[-1]+1)\n",
    "\n",
    "    cur_loss = total_loss/num_items\n",
    "    print('cur_loss',cur_loss)\n",
    "    cur_msgs = get_messages(ogn)\n",
    "    cur_msgs['epoch'] = epoch\n",
    "    cur_msgs['loss'] = cur_loss\n",
    "    messages_over_time.append(cur_msgs)\n",
    "\n",
    "    # cur_self = get_self(ogn)\n",
    "    # cur_self['epoch'] = epoch\n",
    "    # cur_self['loss'] = cur_loss\n",
    "    # self_over_time.append(cur_self)\n",
    "\n",
    "    ogn.cpu()\n",
    "\n",
    "    recorded_models.append(ogn.state_dict()) # it saves #epoch copies of the last model!!!\n",
    "    recorded_times.append(epoch)\n",
    "    recorded_loss.append(cur_loss)\n",
    "\n",
    "    torch.save(ogn.state_dict(), os.path.join(saving_folder, 'epoch-{0}.pth'.format(epoch)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 458
    },
    "id": "lZQ7mjqvzWDT",
    "outputId": "a3f3a129-c0b3-4d2a-d0cf-e33933ff8f1b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>5.750639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>15.865722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>3.199821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>3.199821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.987458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46459</th>\n",
       "      <td>-33.891846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46460</th>\n",
       "      <td>-10.090795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46461</th>\n",
       "      <td>-10.090795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46462</th>\n",
       "      <td>-34.868423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46463</th>\n",
       "      <td>-34.868423</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>46446 rows × 1 columns</p>\n",
       "</div><br><label><b>dtype:</b> float32</label>"
      ],
      "text/plain": [
       "18        5.750639\n",
       "19       15.865722\n",
       "20        3.199821\n",
       "21        3.199821\n",
       "22        0.987458\n",
       "           ...    \n",
       "46459   -33.891846\n",
       "46460   -10.090795\n",
       "46461   -10.090795\n",
       "46462   -34.868423\n",
       "46463   -34.868423\n",
       "Name: x1, Length: 46446, dtype: float32"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages_over_time[0].x1[18:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 158
    },
    "id": "a8Wr93OhzWDU",
    "outputId": "a4735c57-2540-4350-b304-f7b517ff0d94"
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-4026bd806b05>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0monp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself_over_time\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m49\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselfv3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself_over_time\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m49\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselfv3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "onp.sqrt((self_over_time[49][6::10].selfv3)**2 + (self_over_time[49][6::10].selfv3)**2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xxBu9S5bzWDV"
   },
   "outputs": [],
   "source": [
    "self_over_time[49][[\"distv5\", \"distv6\"]]*simulation_data[\"drag_coefficient\"][0,0,0,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SaY2IRxBzWDV"
   },
   "source": [
    "# **Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jJrgIBGUzWDW"
   },
   "outputs": [],
   "source": [
    "\n",
    "#Only turn on one of these:\n",
    "plot_force_components = True\n",
    "plot_sparsity = False\n",
    "plot_rotation = False\n",
    "if plot_force_components:\n",
    "    fig, ax = plt.subplots(1, dim, figsize=(4*dim, 4))\n",
    "if plot_sparsity or plot_rotation:\n",
    "    fig, ax = plt.subplots(1, 1)\n",
    "cam = Camera(fig)\n",
    "\n",
    "\n",
    "last_alpha_x1 = 0.0\n",
    "last_alpha_y1 = 0.0\n",
    "\n",
    "\n",
    "msgs = copy(messages_over_time[0])\n",
    "print('msgs.dtypes',msgs.dtypes[0:13])\n",
    "print('98 messages e1, e2...')\n",
    "print('msgs.dtypes',msgs.dtypes[-6:-1])\n",
    "print('our test daat set has 1000 samples of 12 edges each',np.shape(msgs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HrFSGoI4zWDW"
   },
   "outputs": [],
   "source": [
    "\n",
    "title = 'test'\n",
    "t = lambda _: _#tqdm\n",
    "\n",
    "for i in t(range(0, len(messages_over_time), 10)): #len(messages_over_time) number of epochs\n",
    "    msgs = copy(messages_over_time[i])\n",
    "\n",
    "    msgs['bd'] = msgs.r #+ 1e-2 # distance between two particles, I guess we sum 1e-2 to prevent divisions by 0?\n",
    "\n",
    "    #we take only the messages corresponding to forces\n",
    "    try:\n",
    "        msg_columns = ['e%d'%(k) for k in range(1, msg_dim+1)]\n",
    "        msg_array = np.array(msgs[msg_columns])\n",
    "    except:\n",
    "        msg_columns = ['e%d'%(k) for k in range(msg_dim)]\n",
    "        msg_array = np.array(msgs[msg_columns])\n",
    "\n",
    "    msg_importance = msg_array.std(axis=0)\n",
    "    most_important = np.argsort(msg_importance)[-dim:]\n",
    "    print('most_important',most_important)\n",
    "    msgs_to_compare = msg_array[:, most_important]\n",
    "    msgs_to_compare = (msgs_to_compare - np.average(msgs_to_compare, axis=0)) / np.std(msgs_to_compare, axis=0)\n",
    "\n",
    "    if plot_sparsity:\n",
    "        ax.pcolormesh(msg_importance[np.argsort(msg_importance)[::-1][None, :15]], cmap='gray_r', edgecolors='k')\n",
    "        # plt.colorbar()\n",
    "        plt.axis('off')\n",
    "        plt.grid(True)\n",
    "        ax.set_aspect('equal')\n",
    "        plt.text(15.5, 0.5, '...', fontsize=30)\n",
    "        # fig.suptitle(title + test + 'mse=%.3e'%(min_result.fun/len(msgs),))\n",
    "        plt.tight_layout()\n",
    "\n",
    "    if plot_force_components or plot_rotation:\n",
    "        pos_cols = ['dx', 'dy']\n",
    "        if dim == 3:\n",
    "            pos_cols.append('dz')\n",
    "\n",
    "        sigma = 1.\n",
    "        force_fnc = lambda msg: ((sigma)**12/(np.array(msg.bd)[:, None])**14 - (sigma)**6/(np.array(msg.bd)[:, None])**8) * np.array(msg[pos_cols])\n",
    "        # if sim == 'spring':\n",
    "        #     force_fnc = lambda msg: -np.array(msg.bd - 1)[:, None] * np.array(msg[pos_cols]) / np.array(msg.bd)[:, None]\n",
    "        # elif sim == 'r2':\n",
    "        #     force_fnc = lambda msg: np.array(msg[pos_cols]) / (np.array(msg.bd)[:, None])**3.0 #masses equal to 1\n",
    "        # else:\n",
    "        #     raise NotImplementedError(\"The current force function is for a spring. You will need to change the force function below to that expected by your simulation.\")\n",
    "\n",
    "\n",
    "        expected_forces = force_fnc(msgs)\n",
    "\n",
    "        def percentile_sum(x):\n",
    "            x = x.ravel()\n",
    "            bot = x.min()\n",
    "            top = np.percentile(x, 90)\n",
    "            msk = (x>=bot) & (x<=top)\n",
    "            frac_good = (msk).sum()/len(x)\n",
    "            return x[msk].sum()/frac_good\n",
    "\n",
    "        from scipy.optimize import minimize\n",
    "\n",
    "        def linear_transformation_2d(alpha):\n",
    "\n",
    "            lincomb1 = (alpha[0] * expected_forces[:, 0] + alpha[1] * expected_forces[:, 1]) + alpha[2]\n",
    "            lincomb2 = (alpha[3] * expected_forces[:, 0] + alpha[4] * expected_forces[:, 1]) + alpha[5]\n",
    "\n",
    "            score = (\n",
    "                percentile_sum(np.square(msgs_to_compare[:, 0] - lincomb1)) +\n",
    "                percentile_sum(np.square(msgs_to_compare[:, 1] - lincomb2))\n",
    "            )/2.0\n",
    "\n",
    "            return score\n",
    "\n",
    "        def out_linear_transformation_2d(alpha):\n",
    "            lincomb1 = (alpha[0] * expected_forces[:, 0] + alpha[1] * expected_forces[:, 1]) + alpha[2]\n",
    "            lincomb2 = (alpha[3] * expected_forces[:, 0] + alpha[4] * expected_forces[:, 1]) + alpha[5]\n",
    "\n",
    "            return lincomb1, lincomb2\n",
    "\n",
    "        def linear_transformation_3d(alpha):\n",
    "\n",
    "            lincomb1 = (alpha[0] * expected_forces[:, 0] + alpha[1] * expected_forces[:, 1] + alpha[2] * expected_forces[:, 2]) + alpha[3]\n",
    "            lincomb2 = (alpha[0+4] * expected_forces[:, 0] + alpha[1+4] * expected_forces[:, 1] + alpha[2+4] * expected_forces[:, 2]) + alpha[3+4]\n",
    "            lincomb3 = (alpha[0+8] * expected_forces[:, 0] + alpha[1+8] * expected_forces[:, 1] + alpha[2+8] * expected_forces[:, 2]) + alpha[3+8]\n",
    "\n",
    "            score = (\n",
    "                percentile_sum(np.square(msgs_to_compare[:, 0] - lincomb1)) +\n",
    "                percentile_sum(np.square(msgs_to_compare[:, 1] - lincomb2)) +\n",
    "                percentile_sum(np.square(msgs_to_compare[:, 2] - lincomb3))\n",
    "            )/3.0\n",
    "\n",
    "            return score\n",
    "\n",
    "        def out_linear_transformation_3d(alpha):\n",
    "\n",
    "            lincomb1 = (alpha[0] * expected_forces[:, 0] + alpha[1] * expected_forces[:, 1] + alpha[2] * expected_forces[:, 2]) + alpha[3]\n",
    "            lincomb2 = (alpha[0+4] * expected_forces[:, 0] + alpha[1+4] * expected_forces[:, 1] + alpha[2+4] * expected_forces[:, 2]) + alpha[3+4]\n",
    "            lincomb3 = (alpha[0+8] * expected_forces[:, 0] + alpha[1+8] * expected_forces[:, 1] + alpha[2+8] * expected_forces[:, 2]) + alpha[3+8]\n",
    "\n",
    "            return lincomb1, lincomb2, lincomb3\n",
    "\n",
    "        if dim == 2:\n",
    "            min_result = minimize(linear_transformation_2d, np.ones(dim**2 + dim), method='Powell')\n",
    "        if dim == 3:\n",
    "            min_result = minimize(linear_transformation_3d, np.ones(dim**2 + dim), method='Powell')\n",
    "        print(title, test, 'gets', min_result.fun/len(msgs))\n",
    "\n",
    "        if plot_rotation:\n",
    "            q = min_result.x\n",
    "            alphax1, alphay1, offset1 = q[:3]\n",
    "            alphax2, alphay2, offset2 = q[3:]\n",
    "\n",
    "            s1 = alphax1**2 + alphay1**2\n",
    "            s2 = alphax2**2 + alphay2**2\n",
    "\n",
    "            if (\n",
    "                    (alphax2 - last_alpha_x1)**2\n",
    "                    + (alphay2 - last_alpha_y1)**2  <\n",
    "                   (alphax1 - last_alpha_x1)**2\n",
    "                    + (alphay1 - last_alpha_y1)**2):\n",
    "\n",
    "                alphax1, alphay1, offset1 = q[3:]\n",
    "                alphax2, alphay2, offset2 = q[:3]\n",
    "\n",
    "            last_alpha_x1 = alphax1\n",
    "            last_alpha_y1 = alphay1\n",
    "            s1 = alphax1**2 + alphay1**2\n",
    "            s2 = alphax2**2 + alphay2**2\n",
    "            alphax1 /= s1**0.5 * 2\n",
    "            alphay1 /= s1**0.5 * 2\n",
    "            alphax2 /= s2**0.5 * 2\n",
    "            alphay2 /= s2**0.5 * 2\n",
    "\n",
    "            ax.arrow(0.5, 0.5, alphax1, alphay1, color='k', head_width=0.05, length_includes_head=True)\n",
    "            ax.arrow(0.5, 0.5, alphax2, alphay2, color='k', head_width=0.05, length_includes_head=True)\n",
    "            ax.axis('off')\n",
    "\n",
    "        if plot_force_components:\n",
    "            for i in range(dim):\n",
    "                if dim == 3:\n",
    "                    px = out_linear_transformation_3d(min_result.x)[i]\n",
    "                else:\n",
    "                    px = out_linear_transformation_2d(min_result.x)[i]\n",
    "\n",
    "                py = msgs_to_compare[:, i]\n",
    "                ax[i].scatter(px, py,\n",
    "                              alpha=0.1, s=0.1, color='k')\n",
    "                ax[i].set_xlabel('Linear combination of forces')\n",
    "                ax[i].set_ylabel('Message Element %d'%(i+1))\n",
    "\n",
    "                xlim = np.array([np.percentile(px, q) for q in [10, 90]])\n",
    "                ylim = np.array([np.percentile(py, q) for q in [10, 90]])\n",
    "                xlim[0], xlim[1] = xlim[0] - (xlim[1] - xlim[0])*0.05, xlim[1] + (xlim[1] - xlim[0])*0.05\n",
    "                ylim[0], ylim[1] = ylim[0] - (ylim[1] - ylim[0])*0.05, ylim[1] + (ylim[1] - ylim[0])*0.05\n",
    "\n",
    "                # ax[i].set_xlim(xlim)\n",
    "                # ax[i].set_ylim(ylim)\n",
    "\n",
    "#                 ax[i].set_xlim((-1,1))\n",
    "#                 ax[i].set_ylim((-1,1))\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "    cam.snap()\n",
    "\n",
    "ani = cam.animate()\n",
    "\n",
    "from IPython.display import HTML\n",
    "HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "shfUUrimzWDX"
   },
   "outputs": [],
   "source": [
    "# For CPU\n",
    "\n",
    "from simulate import make_transparent_color\n",
    "from scipy.integrate import odeint\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(8, 4))\n",
    "camera = Camera(fig)\n",
    "\n",
    "\n",
    "for current_model in [0,total_epochs-1]: # current_model<=-2 corresponds to Random model!\n",
    "    i = 1 #4 #2 #Use this simulation\n",
    "    if current_model > total_epochs-1:\n",
    "        continue\n",
    "\n",
    "    #Truth:\n",
    "    cutoff_time =  nt//2  #300\n",
    "    times = onp.array(s.times)[:cutoff_time]\n",
    "    x_times = onp.array(data[i, :cutoff_time])\n",
    "    masses = x_times[:, :, -1]\n",
    "    length_of_tail = 75\n",
    "\n",
    "    #Learned:\n",
    "    e = get_edge_index_cutoff(1,1,d_cutoff_TF=False).cpu() #change by d_cutoff_TF=True???\n",
    "#     ogn.cpu()\n",
    "    if current_model > -2:\n",
    "#         ogn.load_state_dict(recorded_models[current_model])\n",
    "        load_file = os.path.join(saving_folder, 'epoch-{0}.pth'.format(current_model))\n",
    "        print('loading ',load_file)\n",
    "        ogn.load_state_dict(torch.load(load_file))\n",
    "\n",
    "\n",
    "    else:\n",
    "        # Random model!\n",
    "        ogn = OGN(n_f, msg_dim, dim, dt=0.1, hidden=hidden, edge_index=get_edge_index(n, sim), aggr=aggr).cpu()\n",
    "#     ogn_new.cpu()\n",
    "\n",
    "    def odefunc(y, t=None):\n",
    "        y = y.reshape(n, 6).astype(np.float32)\n",
    "        cur = Data(\n",
    "            x=torch.from_numpy(y).cpu(),\n",
    "            edge_index=e\n",
    "        )\n",
    "        dx = y[:, 2:4]\n",
    "        dv = ogn.just_derivative(cur).cpu().detach().numpy()\n",
    "        dother = np.zeros_like(dx)\n",
    "        return np.concatenate((dx, dv, dother), axis=1).ravel()\n",
    "\n",
    "    datai = odeint(odefunc, (onp.asarray(x_times[0]).ravel()), times).reshape(-1, n, 6)\n",
    "    x_times2 = onp.array(datai)\n",
    "\n",
    "    d_idx = 10\n",
    "    for t_idx in range(d_idx, cutoff_time, d_idx):\n",
    "        start = max([0, t_idx-length_of_tail])\n",
    "        ctimes = times[start:t_idx]\n",
    "        cx_times = x_times[start:t_idx]\n",
    "        cx_times2 = x_times2[start:t_idx]\n",
    "        for j in range(n):\n",
    "            rgba = make_transparent_color(len(ctimes), j/n)\n",
    "            ax[0].scatter(cx_times[:, j, 0], cx_times[:, j, 1], color=rgba) #left plot is the true movement\n",
    "            ax[1].scatter(cx_times2[:, j, 0], cx_times2[:, j, 1], color=rgba) #right plot predicted movement\n",
    "            black_rgba = rgba\n",
    "            black_rgba[:, :3] = 0.75\n",
    "            ax[1].scatter(cx_times[:, j, 0], cx_times[:, j, 1], color=black_rgba, zorder=-1) # right plot grey -> true movement\n",
    "\n",
    "        for k in range(2):\n",
    "            ax[k].set_xlim(-3, 3)\n",
    "            ax[k].set_ylim(-3, 3)\n",
    "        plt.tight_layout()\n",
    "        camera.snap()\n",
    "\n",
    "        if t_idx > cutoff_time-d_idx:\n",
    "            fig.title('current_model {0}'.format(current_model))\n",
    "\n",
    "# camera.animate().save('multiple_animations_with_comparison.mp4')\n",
    "from IPython.display import HTML\n",
    "HTML(camera.animate().to_jshtml())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uRyuEDu9zWDY"
   },
   "outputs": [],
   "source": [
    "# messages_over_time = [0,1,2,3,4,5]\n",
    "\n",
    "for i in t(range(0,total_epochs , 1)): #len(messages_over_time) number of epochs\n",
    "\n",
    "\n",
    "    load_file = os.path.join(saving_folder, 'epoch-{0}.pth'.format(i))\n",
    "    print('loading ',load_file)\n",
    "    ogn.load_state_dict(torch.load(load_file))\n",
    "    cur_msgs = get_messages(ogn)\n",
    "\n",
    "    msgs = copy(cur_msgs)\n",
    "\n",
    "    msgs['bd'] = msgs.r + 1e-2 # distance between two particles, I guess we sum 1e-2 to prevent divisions by 0?\n",
    "\n",
    "    #we take only the messages corresponding to forces\n",
    "    try:\n",
    "        msg_columns = ['e%d'%(k) for k in range(1, msg_dim+1)]\n",
    "        msg_array = np.array(msgs[msg_columns])\n",
    "    except:\n",
    "        msg_columns = ['e%d'%(k) for k in range(msg_dim)]\n",
    "        msg_array = np.array(msgs[msg_columns])\n",
    "\n",
    "    msg_importance = msg_array.std(axis=0)\n",
    "    most_important = np.argsort(msg_importance)[-dim:]\n",
    "    print('most_important',most_important)\n",
    "    msgs_to_compare = msg_array[:, most_important]\n",
    "    msgs_to_compare = (msgs_to_compare - np.average(msgs_to_compare, axis=0)) / np.std(msgs_to_compare, axis=0)\n",
    "\n",
    "\n",
    "newtestloader_2 = DataLoader(\n",
    "    [Data(\n",
    "        X_test[i],\n",
    "        edge_index=get_edge_index_cutoff(X_test[i],d_cutoff,d_cutoff_TF=False),\n",
    "        y=y_test[i]) for i in test_idxes],\n",
    "    batch_size=len(X_test),\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "print(np.shape(X_test))\n",
    "\n",
    "i=0\n",
    "for ginput_plot_all in newtestloader_2:     # Trainloader has the 1500 samples (pos vel edge index, ...).\n",
    "    print('i ', i)\n",
    "    if i >= 2:\n",
    "        print('break')\n",
    "        break\n",
    "\n",
    "    print('number of messages if fully connected graphs: ', num_graphs_test*n*(n-1))\n",
    "    print(np.shape(ginput_plot_all.edge_index))\n",
    "    i+=1\n",
    "\n",
    "\n",
    "\n",
    "newtestloader_3 = DataLoader(\n",
    "    [Data(\n",
    "        X_test[i],\n",
    "        edge_index=get_edge_index_cutoff(X_test[i],d_cutoff,d_cutoff_TF=d_cutoff_TF),\n",
    "        y=y_test[i]) for i in test_idxes],\n",
    "    batch_size=len(X_test),\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "print(np.shape(X_test))\n",
    "\n",
    "i=0\n",
    "for ginput_plot_cutoff in newtestloader_3:     # Trainloader has the 1500 samples (pos vel edge index, ...).\n",
    "    print('i ', i)\n",
    "    if i >= 2:\n",
    "        print('break')\n",
    "        break\n",
    "\n",
    "    print('number of messages if fully connected graphs: ', num_graphs_test*n*(n-1))\n",
    "    print(np.shape(ginput_plot_cutoff.edge_index))\n",
    "    i+=1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o1VBMagrzWDZ"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# TRYING TO PLOT THE FORCE\n",
    "\n",
    "def plotting_force(ginput_plot, epoch_pl_list = [0,2]):\n",
    "\n",
    "    s1 = ginput_plot.x[ginput_plot.edge_index[0,:]]\n",
    "    s2 = ginput_plot.x[ginput_plot.edge_index[1,:]]\n",
    "\n",
    "    print('np.shape(s1)',np.shape(s1))\n",
    "    print('np.shape(s2)',np.shape(s2))\n",
    "\n",
    "    distance = (s1[:,0:2]-s2[:,0:2]).detach().numpy()\n",
    "    print('np.shape(distance)',np.shape(distance))\n",
    "    distance_mod = np.linalg.norm(distance,axis=1)\n",
    "    print('np.shape(distance_mod)',np.shape(distance_mod))\n",
    "\n",
    "\n",
    "    fig, ax1 = plt.subplots()\n",
    "\n",
    "    color = 'tab:red'\n",
    "    ax1.set_xlabel('distance')\n",
    "    ax1.set_ylabel('count', color=color)\n",
    "    logbins = np.logspace(np.log10(min(distance_mod)),np.log10(max(distance_mod)),50)\n",
    "    ax1.hist(distance_mod, bins = logbins, color=color)\n",
    "    ax1.set_xscale('log')\n",
    "    ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "    ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "\n",
    "    color = 'tab:blue'\n",
    "\n",
    "    x_pl = np.linspace(0,10,100)\n",
    "    y_pl = fermi_func(x_pl**2)\n",
    "    ax2.plot(x_pl,y_pl)\n",
    "    ax2.set_ylabel('Prob considering edge', color=color)  # we already handled the x-label with ax1\n",
    "    ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "    ax2.set_xlim((0.001,100))\n",
    "    fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "    plt.show()\n",
    "\n",
    "    for epoch_pl in epoch_pl_list:\n",
    "\n",
    "        load_file = os.path.join(saving_folder, 'epoch-{0}.pth'.format(epoch_pl))\n",
    "        print('loading ',load_file)\n",
    "        ogn.load_state_dict(torch.load(load_file))\n",
    "        print('ogn.edge_index[0]',ogn.edge_index[0,0])\n",
    "\n",
    "        force = ogn.message(s1, s2)[:,most_important].detach().numpy()\n",
    "        force_mod = np.linalg.norm(force,axis=1)\n",
    "        print('np.shape(force_mod)',np.shape(force_mod))\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(distance_mod,force_mod,'.')\n",
    "        xxx = np.linspace(0.1, 1, 10)\n",
    "        yyy = 1/xxx**2\n",
    "        plt.plot(xxx,yyy,'--')\n",
    "\n",
    "        ax = plt.gca()\n",
    "        ax.set_xscale('log')\n",
    "        ax.set_yscale('log')\n",
    "        plt.xlabel('distance')\n",
    "        plt.ylabel('force')\n",
    "        plt.xlim((0.001,100))\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "plotting_force(ginput_plot_all, epoch_pl_list = [0,5])# [0,total_epochs-1])\n",
    "plotting_force(ginput_plot_cutoff, epoch_pl_list = [0,5])# [0,total_epochs-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Oh6640FMzWDa"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# TRYING TO PLOT THE FORCE\n",
    "\n",
    "def plotting_force():\n",
    "\n",
    "#     title = '{}_n={}_dim={}_nt={}_dt={}'.format(sim, n, dim, nt, dt)\n",
    "#     saving_folder = title+'_epochs_'+'{0}_d_cutoff_{1}'.format(total_epochs,d_cutoff_TF)\n",
    "\n",
    "    load_file = os.path.join(saving_folder, 'epoch-{0}.pth'.format(0))\n",
    "    print('loading ',load_file)\n",
    "    ogn.load_state_dict(torch.load(load_file))\n",
    "\n",
    "    print('ogn.edge_index[0]',ogn.edge_index[0,0])\n",
    "\n",
    "    s1 = ginput_plot.x[ginput_plot.edge_index[0,:]]\n",
    "    s2 = ginput_plot.x[ginput_plot.edge_index[1,:]]\n",
    "\n",
    "    print('np.shape(s1)',np.shape(s1))\n",
    "    print('np.shape(s2)',np.shape(s2))\n",
    "\n",
    "    distance = (s1[:,0:2]-s2[:,0:2]).detach().numpy()\n",
    "    print('np.shape(distance)',np.shape(distance))\n",
    "    distance_mod = np.linalg.norm(distance,axis=1)\n",
    "    print('np.shape(distance_mod)',np.shape(distance_mod))\n",
    "\n",
    "    # histogram of the data\n",
    "    plt.figure()\n",
    "    logbins = np.logspace(np.log10(min(distance_mod)),np.log10(max(distance_mod)),20)\n",
    "    plt.hist(distance_mod, bins = logbins)\n",
    "    plt.xscale('log')\n",
    "    plt.xlabel('distance')\n",
    "    plt.ylabel('count')\n",
    "    plt.show()\n",
    "\n",
    "    # cutoff\n",
    "\n",
    "    if d_cutoff_TF == 'smooth':\n",
    "\n",
    "        print('we remove edges of particles following this probability:')\n",
    "        plt.figure()\n",
    "        x_pl = np.linspace(0,10,100)\n",
    "        y_pl = fermi_func(x_pl)\n",
    "        plt.plot(x_pl,y_pl)\n",
    "        plt.xscale('log')\n",
    "        plt.xlabel('distance square')\n",
    "        plt.ylabel('Prob considering edge')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "\n",
    "    fig, ax1 = plt.subplots()\n",
    "\n",
    "    color = 'tab:red'\n",
    "    ax1.set_xlabel('distance')\n",
    "    ax1.set_ylabel('count', color=color)\n",
    "    logbins = np.logspace(np.log10(min(distance_mod)),np.log10(max(distance_mod)),20)\n",
    "    ax1.hist(distance_mod, bins = logbins, color=color)\n",
    "    ax1.set_xscale('log')\n",
    "    ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "    ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "\n",
    "    color = 'tab:blue'\n",
    "\n",
    "    x_pl = np.linspace(0,10,100)\n",
    "    y_pl = fermi_func(x_pl)\n",
    "    ax2.plot(x_pl,y_pl)\n",
    "    ax2.set_ylabel('Prob considering edge', color=color)  # we already handled the x-label with ax1\n",
    "    ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "    fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "#     force = ogn.message(s1, s2)[:,most_important].detach().numpy()\n",
    "#     force_mod = np.linalg.norm(force,axis=1)\n",
    "#     print('np.shape(force_mod)',np.shape(force_mod))\n",
    "\n",
    "#     plt.figure()\n",
    "#     plt.plot(distance_mod,force_mod,'.')\n",
    "#     xxx = np.linspace(0.1, 1, 10)\n",
    "#     yyy = 1/xxx**2\n",
    "#     plt.plot(xxx,yyy,'--')\n",
    "\n",
    "#     ax = plt.gca()\n",
    "#     ax.set_xscale('log')\n",
    "#     ax.set_yscale('log')\n",
    "#     plt.xlabel('distance')\n",
    "#     plt.ylabel('force')\n",
    "#     plt.show()\n",
    "\n",
    "#     plot_epoch = 2 #total_epochs-1\n",
    "\n",
    "#     print('epoch ', plot_epoch)\n",
    "\n",
    "#     load_file = os.path.join(saving_folder, 'epoch-{0}.pth'.format(plot_epoch))\n",
    "#     print('loading ',load_file)\n",
    "#     ogn.load_state_dict(torch.load(load_file))\n",
    "\n",
    "#     print('ogn.edge_index[0]',ogn.edge_index[0,0])\n",
    "\n",
    "#     s1 = ginput_plot.x[ginput_plot.edge_index[0,:]]\n",
    "#     s2 = ginput_plot.x[ginput_plot.edge_index[1,:]]\n",
    "\n",
    "#     print('np.shape(s1)',np.shape(s1))\n",
    "#     print('np.shape(s2)',np.shape(s2))\n",
    "\n",
    "#     distance = (s1[:,0:2]-s2[:,0:2]).detach().numpy()\n",
    "#     print('np.shape(distance)',np.shape(distance))\n",
    "#     distance_mod = np.linalg.norm(distance,axis=1)\n",
    "#     print('np.shape(distance_mod)',np.shape(distance_mod))\n",
    "\n",
    "\n",
    "#     force = ogn.message(s1, s2)[:,most_important].detach().numpy()\n",
    "#     force_mod = np.linalg.norm(force,axis=1)\n",
    "#     print('np.shape(force_mod)',np.shape(force_mod))\n",
    "\n",
    "#     plt.figure()\n",
    "#     plt.plot(distance_mod,force_mod,'.')\n",
    "#     xxx = np.linspace(0.1, 1, 10)\n",
    "#     yyy = 1/xxx**2\n",
    "#     plt.plot(xxx,yyy,'--')\n",
    "\n",
    "#     ax = plt.gca()\n",
    "#     ax.set_xscale('log')\n",
    "#     ax.set_yscale('log')\n",
    "#     plt.xlabel('distance')\n",
    "#     plt.ylabel('force')\n",
    "#     plt.xlim((0.001,10))\n",
    "#     plt.show()\n",
    "\n",
    "\n",
    "plotting_force()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yzAfuBXAzWDb"
   },
   "outputs": [],
   "source": [
    "# # For GPU\n",
    "\n",
    "# fig, ax = plt.subplots(1, 2, figsize=(8, 4))\n",
    "# camera = Camera(fig)\n",
    "\n",
    "# for current_model in [-1] + [1, 34, 67, 100, 133, 166, 199]:\n",
    "#     i = 4 #Use this simulation\n",
    "#     if current_model > len(recorded_models):\n",
    "#         continue\n",
    "\n",
    "#     #Truth:\n",
    "#     cutoff_time = 300\n",
    "#     times = onp.array(s.times)[:cutoff_time]\n",
    "#     x_times = onp.array(data[i, :cutoff_time])\n",
    "#     masses = x_times[:, :, -1]\n",
    "#     length_of_tail = 75\n",
    "\n",
    "#     #Learned:\n",
    "#     e = edge_index.cuda()\n",
    "#     ogn.cpu()\n",
    "#     if current_model > -1:\n",
    "#         ogn.load_state_dict(recorded_models[current_model])\n",
    "#     else:\n",
    "#         # Random model!\n",
    "#         ogn = OGN(n_f, msg_dim, dim, dt=0.1, hidden=hidden, edge_index=get_edge_index(n, sim), aggr=aggr).cuda()\n",
    "#     ogn.cuda()\n",
    "\n",
    "#     def odefunc(y, t=None):\n",
    "#         y = y.reshape(n, 6).astype(np.float32)\n",
    "#         cur = Data(\n",
    "#             x=torch.from_numpy(y).cuda(),\n",
    "#             edge_index=e\n",
    "#         )\n",
    "#         dx = y[:, 2:4]\n",
    "#         dv = ogn.just_derivative(cur).cpu().detach().numpy()\n",
    "#         dother = np.zeros_like(dx)\n",
    "#         return np.concatenate((dx, dv, dother), axis=1).ravel()\n",
    "\n",
    "#     datai = odeint(odefunc, (onp.asarray(x_times[0]).ravel()), times).reshape(-1, n, 6)\n",
    "#     x_times2 = onp.array(datai)\n",
    "\n",
    "#     d_idx = 10\n",
    "#     for t_idx in range(d_idx, cutoff_time, d_idx):\n",
    "#         start = max([0, t_idx-length_of_tail])\n",
    "#         ctimes = times[start:t_idx]\n",
    "#         cx_times = x_times[start:t_idx]\n",
    "#         cx_times2 = x_times2[start:t_idx]\n",
    "#         for j in range(n):\n",
    "#             rgba = make_transparent_color(len(ctimes), j/n)\n",
    "#             ax[0].scatter(cx_times[:, j, 0], cx_times[:, j, 1], color=rgba)\n",
    "#             ax[1].scatter(cx_times2[:, j, 0], cx_times2[:, j, 1], color=rgba)\n",
    "#             black_rgba = rgba\n",
    "#             black_rgba[:, :3] = 0.75\n",
    "#             ax[1].scatter(cx_times[:, j, 0], cx_times[:, j, 1], color=black_rgba, zorder=-1)\n",
    "\n",
    "#         for k in range(2):\n",
    "#             ax[k].set_xlim(-1, 3)\n",
    "#             ax[k].set_ylim(-3, 1)\n",
    "#         plt.tight_layout()\n",
    "#         camera.snap()\n",
    "\n",
    "# # camera.animate().save('multiple_animations_with_comparison.mp4')\n",
    "# from IPython.display import HTML\n",
    "# HTML(camera.animate().to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-VvBR-_tzWDb"
   },
   "outputs": [],
   "source": [
    "#######################\n",
    "# Symbolic regression #\n",
    "#######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "doY9oHvbzWDc"
   },
   "outputs": [],
   "source": [
    "# Finally, one can install a free 30-day trial of Eureqa at this URL: https://www.nutonian.com/download/eureqa-desktop-download/.\n",
    "\n",
    "# Extract the force laws with the following procedure:\n",
    "\n",
    "#     The data in messages_over_time correspond to inputs to, and features of, ϕe, recorded during each training epoch.\n",
    "#     Select the last element of this list.\n",
    "#     Find the most significant message feature. Each message feature corresponds to 'e1', 'e2', etc. Calculate the one with the largest standard deviation.\n",
    "#     Save that message to csv file along with the input features. Paste these into the data column of Eureqa (it's a GUI app).\n",
    "#     Don't change anything on the \"prepare data\" tab.\n",
    "#     Enable your choice of operators with complexity levels for each (this choice is up to you).\n",
    "#     Set the loss to mean absolute error (the default) and begin training.\n",
    "#     After you start training, you will be able to see a table of equations as a function of complexity and fit to the data. These represent a symbolic representation of the message function.\n",
    "#     Find the equation that has the greatest drop in log mean absolute error over increase in complexity. This equation should be equal to a linear transform of the true force.\n",
    "\n",
    "# Thus, we have extracted a force law from the graph network without priors on the functional form.\n",
    "\n",
    "# This is the same technique we used to extract the unknown dark matter overdensity equation from the Quijote simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t6LM6csDzWDd"
   },
   "outputs": [],
   "source": [
    "# Here's the best message, which we will study:\n",
    "best_message = np.argmax([np.std(messages_over_time[-1]['e%d'%(i,)]) for i in range(100)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AGdSLVs4zWDe"
   },
   "outputs": [],
   "source": [
    "# Here's a pandas dataframe of the message data:\n",
    "messages_over_time[-1][['e%d'%(best_message,), 'dx', 'dy', 'r', 'm1', 'm2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zGBiqewfzWDe"
   },
   "outputs": [],
   "source": [
    "# Now we just fit e4 as a function of dx, dy, r, m1, and m2, inside Eureqa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J5ZDRMGHzWDf"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "abp_inference",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
